{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijGzTHJJUCPY"
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VEqbX8OhE8y9"
   },
   "source": [
    "# Usando Gemini para escenarios educativos\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/education/use_cases_for_education.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Ejecutar en Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/education/use_cases_for_education.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> Ver en GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/education/use_cases_for_education.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Abrir en Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VK1Q5ZYdVL4Y"
   },
   "source": [
    "## Visión general\n",
    "\n",
    "### Gemini\n",
    "\n",
    "Gemini es una familia de modelos de IA generativa desarrollados por Google DeepMind y diseñados para casos de uso multimodales. La API de Gemini proporciona acceso a los modelos Gemini Pro Vision y Gemini Pro.\n",
    "\n",
    "### API de Gemini de Vertex AI\n",
    "\n",
    "La API Vertex AI Gemini proporciona una interfaz unificada para interactuar con los modelos Gemini. Actualmente hay dos modelos disponibles en la API de Gemini:\n",
    "\n",
    "- **Modelo Gemini Pro** (`gemini-pro`): Diseñado para manejar tareas de lenguaje natural, chat de texto y código de múltiples turnos y generación de código.\n",
    "- **Modelo Gemini Pro Vision** (`gemini-pro-vision`): admite indicaciones multimodales. Puede incluir texto, imágenes y videos en sus solicitudes rápidas y obtener respuestas en texto o código.\n",
    "\n",
    "Puede interactuar con la API de Gemini utilizando los siguientes métodos:\n",
    "\n",
    "- Utilice [Vertex AI Studio](https://cloud.google.com/generative-ai-studio) para realizar pruebas rápidas y generar comandos\n",
    "- Utilice el SDK de Vertex AI\n",
    "\n",
    "Este noteobok se centra en el uso del **Vertex AI SDK para Python** para llamar a la API Vertex AI Gemini.\n",
    "\n",
    "Para obtener más información, consulte la documentación [IA generativa en Vertex AI] (https://cloud.google.com/vertex-ai/docs/generative-ai/learn/overview)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVL_vGs4q3pg"
   },
   "source": [
    "### Objetivos\n",
    "\n",
    "El objetivo principal de este notebook es demostrar una variedad de casos de uso educativo que pueden beneficiarse de los modelos Gemini.\n",
    "\n",
    "Los pasos tomados incluyen:\n",
    "\n",
    "- Instalación del SDK de Python\n",
    "- Uso de la API Vertex AI Gemini\n",
    "    - Usando el modelo de texto (`gemini-pro`)\n",
    "      - Razonamiento a diferentes niveles.\n",
    "      - Razonamiento sobre el texto.\n",
    "      - Razonamiento sobre números.\n",
    "    - Utilizando el modelo multimodal (`gemini-pro-vision`)\n",
    "      - Razonamiento sobre una sola imagen.\n",
    "      - Razonamiento sobre múltiples imágenes.\n",
    "      - Razonamiento en un vídeo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bRdtKLfTsQ27"
   },
   "source": [
    "### Costos\n",
    "\n",
    "Este tutorial utiliza los siguientes componentes de Google Cloud que pueden generar cargos en su factura:\n",
    "\n",
    "- Vertex AI\n",
    "\n",
    "Obtenga más información sobre [precios de Vertex AI](https://cloud.google.com/vertex-ai/pricing) y utilice la [calculadora de precios](https://cloud.google.com/products/calculator/) para generar una estimación de costos basada en el uso proyectado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDU0XJ1xRDlL"
   },
   "source": [
    "## Primeros pasos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SBGrQE22sVrt"
   },
   "source": [
    "### Instalar el SDK de Vertex AI\n",
    "**Importante:** solo descomente la línea a continuación si **no** está ejecutando esta práctica de laboratorio en Qwiklabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-hqq5vomsW_P",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! pip3 install --upgrade --user google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Fom0ZkMSBW6"
   },
   "source": [
    "### **Reinicie el kernel de su Jupyter notebook**\n",
    "\n",
    "Como la instalación se realiza con la opción `--user`, es necesario reiniciar el kernel para que los nuevos módulos sean accesibles.\n",
    "\n",
    "**Importante:** solo descomente la línea a continuación si **no** está ejecutando esta práctica de laboratorio en Qwiklabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LCaCx6PLSBW6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import IPython\n",
    "\n",
    "# app = IPython.Application.instance()\n",
    "# app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ Se está reiniciando el kernel del notebook. Espere a que se complete este proceso antes de continuar con los siguientes pasos. ⚠️</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5afkyDMSBW5"
   },
   "source": [
    "### **Solo para uso en Colab: autentique tu notebook**\n",
    "\n",
    "En caso de que estés ejecutando este notebook en Google Colab, descomente esta celda para realizar la autenticación de tu sesión de notebook con Google Cloud. Este paso es importante **para utilizar no Colab** para garantizar que las API de Google Cloud funcionen Sem problemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ab4Y6eSIUknb"
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "\n",
    "# # Additional authentication is required for Google Colab\n",
    "# if \"google.colab\" in sys.modules:\n",
    "#     # Authenticate user to Google Cloud\n",
    "#     from google.colab import auth\n",
    "\n",
    "#     auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kc4WxYmLSBW5"
   },
   "source": [
    "### **Solo para uso en Colab: define el proyecto Google Cloud para ser utilizado**\n",
    "\n",
    "Si estás ejecutando este notebook en Google Colab, descomente esta celda baja para definir qué proyecto Google Cloud se utilizará en Colab en la ejecución de este notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YmY9HVVGSBW5"
   },
   "outputs": [],
   "source": [
    "# if \"google.colab\" in sys.modules:\n",
    "#     # Define project information\n",
    "#     PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
    "#     LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "\n",
    "#     # Initialize Vertex AI\n",
    "#     import vertexai\n",
    "\n",
    "#     vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kdINrwJZsj1d"
   },
   "source": [
    "### Importando las bibliotecas necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 324,
     "status": "ok",
     "timestamp": 1702337383496,
     "user": {
      "displayName": "Laurent Picard",
      "userId": "17424629147771078746"
     },
     "user_tz": -60
    },
    "id": "stNmWCsRsotM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "from vertexai.generative_models import (\n",
    "    GenerationConfig,\n",
    "    GenerationResponse,\n",
    "    GenerativeModel,\n",
    "    Image,\n",
    "    Part,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0v63w6fWs9Dx"
   },
   "source": [
    "### Definir algunas funciones auxiliares\n",
    "\n",
    "Defina funciones auxiliares para cargar y mostrar imágenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1702337383496,
     "user": {
      "displayName": "Laurent Picard",
      "userId": "17424629147771078746"
     },
     "user_tz": -60
    },
    "id": "OGvJLH4DmZfw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import http.client\n",
    "import io\n",
    "import typing\n",
    "import urllib.request\n",
    "\n",
    "import IPython.display\n",
    "from PIL import Image as PIL_Image\n",
    "from PIL import ImageOps as PIL_ImageOps\n",
    "\n",
    "Contents = str | list[str | Image | Part]\n",
    "\n",
    "\n",
    "def generate_content(\n",
    "    model: GenerativeModel,\n",
    "    contents: Contents,\n",
    "    temperature: float = 0.0,\n",
    "    top_p: float = 0.0,\n",
    "    top_k: int = 1,\n",
    ") -> list[GenerationResponse]:\n",
    "    generation_config = GenerationConfig(\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        candidate_count=1,\n",
    "        max_output_tokens=2048,\n",
    "    )\n",
    "\n",
    "    responses = model.generate_content(\n",
    "        contents,\n",
    "        generation_config=generation_config,\n",
    "        stream=True,\n",
    "    )\n",
    "    return [responses] if isinstance(responses, GenerationResponse) else list(responses)\n",
    "\n",
    "\n",
    "def print_contents(contents: Contents):\n",
    "    if not isinstance(contents, list):\n",
    "        contents = [contents]\n",
    "\n",
    "    print(\" Contenido \".center(80, \"-\"))\n",
    "    for content in contents:\n",
    "        if display_content_as_image(content):\n",
    "            continue\n",
    "        if display_content_as_video(content):\n",
    "            continue\n",
    "        print(content)\n",
    "\n",
    "\n",
    "def display_content_as_image(content: str | Image | Part) -> bool:\n",
    "    if not isinstance(content, Image):\n",
    "        return False\n",
    "    display_image(content)\n",
    "    return True\n",
    "\n",
    "\n",
    "def display_content_as_video(content: str | Image | Part) -> bool:\n",
    "    if not isinstance(content, Part):\n",
    "        return False\n",
    "    part = typing.cast(Part, content)\n",
    "    file_path = part.file_data.file_uri.removeprefix(\"gs://\")\n",
    "    video_url = f\"https://storage.googleapis.com/{file_path}\"\n",
    "    IPython.display.display(IPython.display.Video(video_url, width=600))\n",
    "    return True\n",
    "\n",
    "\n",
    "def print_responses(responses: list[GenerationResponse], as_markdown: bool = True):\n",
    "    # Consolidate the text\n",
    "    text = \"\".join(\n",
    "        part.text\n",
    "        for response in responses\n",
    "        for part in response.candidates[0].content.parts\n",
    "    )\n",
    "    # Remove potential leading/trailing spaces\n",
    "    text = text.strip()\n",
    "\n",
    "    print(\" Inicio de respuestas \".center(80, \"-\"))\n",
    "    if as_markdown:\n",
    "        IPython.display.display(IPython.display.Markdown(text))\n",
    "    else:\n",
    "        print(text)\n",
    "    print(\" Fin de las respuestas \".center(80, \"-\"))\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "def display_image(image: Image, max_width: int = 600, max_height: int = 350):\n",
    "    pil_image = typing.cast(PIL_Image.Image, image._pil_image)\n",
    "    if pil_image.mode != \"RGB\":\n",
    "        # Modes such as RGBA are not yet supported by all Jupyter environments\n",
    "        pil_image = pil_image.convert(\"RGB\")\n",
    "\n",
    "    image_width, image_height = pil_image.size\n",
    "    if max_width < image_width or max_height < image_height:\n",
    "        # Resize to display a smaller notebook image\n",
    "        pil_image = PIL_ImageOps.contain(pil_image, (max_width, max_height))\n",
    "\n",
    "    display_image_compressed(pil_image)\n",
    "\n",
    "\n",
    "def display_image_compressed(pil_image: PIL_Image.Image):\n",
    "    \"\"\"Display the image in a compressed format to reduce the notebook size.\"\"\"\n",
    "    image_io = io.BytesIO()\n",
    "    pil_image.save(image_io, \"jpeg\", quality=80, optimize=True)\n",
    "    image_bytes = image_io.getvalue()\n",
    "    ipython_image = IPython.display.Image(image_bytes)\n",
    "    IPython.display.display(ipython_image)\n",
    "\n",
    "\n",
    "def load_image_from_url(image_url: str) -> Image:\n",
    "    image_bytes = get_image_bytes_from_url(image_url)\n",
    "    return Image.from_bytes(image_bytes)\n",
    "\n",
    "\n",
    "def get_image_bytes_from_url(image_url: str) -> bytes:\n",
    "    with urllib.request.urlopen(image_url) as response:\n",
    "        response = typing.cast(http.client.HTTPResponse, response)\n",
    "        image_bytes = response.read()\n",
    "    return image_bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9R3nV5bE22Z"
   },
   "source": [
    "## Importar el modelo `Gemini 1.0 Pro`\n",
    "\n",
    "Gemini Pro (`gemini-1.0-pro`) lo ayuda a realizar tareas utilizando lenguaje natural, chats de código y texto de varios turnos y generación de código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1702337383834,
     "user": {
      "displayName": "Laurent Picard",
      "userId": "17424629147771078746"
     },
     "user_tz": -60
    },
    "id": "UnUmflwsE22a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = GenerativeModel(\"gemini-1.0-pro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9fm71OTvpyqD"
   },
   "source": [
    "### *Razonamiento* a diferentes niveles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgf3t9odFlIj"
   },
   "source": [
    "Puedes hacer preguntas directas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 64
    },
    "executionInfo": {
     "elapsed": 965,
     "status": "ok",
     "timestamp": 1702337384797,
     "user": {
      "displayName": "Laurent Picard",
      "userId": "17424629147771078746"
     },
     "user_tz": -60
    },
    "id": "tBpDHLmMv2un",
    "outputId": "da0a9444-7d16-4522-ee30-76eec7ee8f2a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "contents = \"\"\"\n",
    "Que les paso a los dinosaurios? ¿Cuando?\n",
    "Explíquelo simplemente en una oración.\n",
    "\"\"\"\n",
    "\n",
    "responses = generate_content(model, contents)\n",
    "print_contents(contents)\n",
    "print_responses(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rfs1qLAoFrZu"
   },
   "source": [
    "…así como preguntas que requieren respuestas más matizadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 586
    },
    "executionInfo": {
     "elapsed": 6374,
     "status": "ok",
     "timestamp": 1702337391168,
     "user": {
      "displayName": "Laurent Picard",
      "userId": "17424629147771078746"
     },
     "user_tz": -60
    },
    "id": "AcVfT9bowUKW",
    "outputId": "64df65b5-06c7-4c2b-9434-e9b32106f422",
    "tags": []
   },
   "outputs": [],
   "source": [
    "contents = \"\"\"\n",
    "¿Estamos 100% seguros de lo que pasó con los dinosaurios?\n",
    "En caso contrario, detalle las principales hipótesis actuales.\n",
    "\"\"\"\n",
    "\n",
    "responses = generate_content(model, contents)\n",
    "print_contents(contents)\n",
    "print_responses(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-KPNK1hNGCk4"
   },
   "source": [
    "Puedes realizar interacciones pidiendo respuestas simples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 114
    },
    "executionInfo": {
     "elapsed": 1251,
     "status": "ok",
     "timestamp": 1702337392408,
     "user": {
      "displayName": "Laurent Picard",
      "userId": "17424629147771078746"
     },
     "user_tz": -60
    },
    "id": "SPoEA7UUqlvy",
    "outputId": "5e3695e0-a571-44b5-9b5d-51f747d6ec47",
    "tags": []
   },
   "outputs": [],
   "source": [
    "contents = \"\"\"\n",
    "Explique por qué es invierno en Francia y verano en Australia.\n",
    "Explica de forma lúdica y divertida, para que un niño de 4 años pueda entender.\n",
    "Responda incluyendo las 3 razones principales.\n",
    "\"\"\"\n",
    "\n",
    "responses = generate_content(model, contents)\n",
    "print_contents(contents)\n",
    "print_responses(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gMlbj1-pGKkA"
   },
   "source": [
    "…o pidiendo respuestas más detalladas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "executionInfo": {
     "elapsed": 3762,
     "status": "ok",
     "timestamp": 1702337396166,
     "user": {
      "displayName": "Laurent Picard",
      "userId": "17424629147771078746"
     },
     "user_tz": -60
    },
    "id": "LdiaJBtbpXjM",
    "outputId": "578bc987-533d-4153-f0ce-528f8567718d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "contents = \"\"\"\n",
    "Explica por qué tenemos mareas.\n",
    "Proporcione una respuesta detallada utilizando viñetas.\n",
    "\"\"\"\n",
    "\n",
    "responses = generate_content(model, contents)\n",
    "print_contents(contents)\n",
    "print_responses(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hV6ULv8KHkxO"
   },
   "source": [
    "Puedes hacer preguntas con un alcance muy cerrado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 446
    },
    "executionInfo": {
     "elapsed": 3798,
     "status": "ok",
     "timestamp": 1702337399959,
     "user": {
      "displayName": "Laurent Picard",
      "userId": "17424629147771078746"
     },
     "user_tz": -60
    },
    "id": "dGk1uEPN0XXq",
    "outputId": "489716c2-c28b-4b0a-ab0b-c6523d8ed676",
    "tags": []
   },
   "outputs": [],
   "source": [
    "contents = \"\"\"\n",
    "¿Cuándo fueron los dos últimos años bisiestos?\n",
    "Enumere dos competencias internacionales que tuvieron lugar durante el penúltimo.\n",
    "\"\"\"\n",
    "\n",
    "responses = generate_content(model, contents)\n",
    "print_contents(contents)\n",
    "print_responses(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SzmiMEq_HsNF"
   },
   "source": [
    "…y también más preguntas abiertas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "executionInfo": {
     "elapsed": 6829,
     "status": "ok",
     "timestamp": 1702337406783,
     "user": {
      "displayName": "Laurent Picard",
      "userId": "17424629147771078746"
     },
     "user_tz": -60
    },
    "id": "HqcYe_w5BvLT",
    "outputId": "176f5e86-d787-46cb-eb43-47ffbf74a79d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "contents = \"\"\"\n",
    "Que fue primero, la gallina o el huevo? Explica desde 3 perspectivas diferentes.\n",
    "¿Cómo llamamos el problema del “huevo y la gallina”? Dé un ejemplo que pueda ocurrir en la educación.\n",
    "\"\"\"\n",
    "\n",
    "responses = generate_content(model, contents)\n",
    "print_contents(contents)\n",
    "print_responses(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BoqqOTjfL6HX"
   },
   "source": [
    "### *Razonamiento* con textos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1dEt9vXqmGN5"
   },
   "source": [
    "Puede utilizar Gemini para realizar resúmenes y traducciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168
    },
    "executionInfo": {
     "elapsed": 1899,
     "status": "ok",
     "timestamp": 1702337408678,
     "user": {
      "displayName": "Laurent Picard",
      "userId": "17424629147771078746"
     },
     "user_tz": -60
    },
    "id": "jAbIQ5U3mGN5",
    "outputId": "d9183b50-3125-42b3-e6ba-dcd0c573767a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "contents = \"\"\"\n",
    "Resume el siguiente texto en tres oraciones, en español, utilizando únicamente el texto.\n",
    "\n",
    "TEXTO:\n",
    "- Les hommes naissent et demeurent libres et égaux en droits. Las distinciones sociales no pueden ser fondées que sur l'utilité commune.\n",
    "- Le but de toute Association Politique est la conservación de los derechos naturales e imprescriptibles del hombre. Estos derechos son la libertad, la propiedad, la seguridad y la resistencia a la opresión.\n",
    "- Le prince de toute souveraineté réside essentiellement dans la Nation. Nul corps, nul individu ne peut exercis d'autorité qui n'en émane expressément.\n",
    "- La libertad consiste en poder hacer todo lo que no pasa en autrui: además, el ejercicio de los derechos naturales de cada hombre n'a de bornes que celles qui surent aux autres membres de la société la jouissance de ces mêmes droits. Estos nacidos no pueden ser determinados por la ley.\n",
    "No es una defensa que las acciones nuisibles à la société. Todo esto que n'est pas defendido por la ley ne peut être empêché, et nul ne peut être contraint à faire ce qu'elle n'ordonne pas.\n",
    "- La ley es la expresión de la voluntad general. Todos los ciudadanos tienen derecho a concurrir con el personal o con sus representantes en la formación. Elle doit être la même pour todosus, soit qu'elle protect, soit qu'elle punisse. Todos los citoyens, étant égaux à ses yeux, sont également admisibles à toutes dignités, place et emplois publics, selon leur capacité et sans autre distinción que celle de leurs vertus et de leurs talentos.\n",
    "- Nul homme ne peut être accusé, arrêté ou détenu que dans les cas determinados par la loi et selon les formes qu'elle a prescrites. Ceux qui sollicitent, expédient, exécutent ou font exécuter des ordres arbitraires doivent être punis; mais tout cytoyen appelé ou saisi en vertu de la loi doit obéir à l'instant ; il se rendición acoplable por la resistencia.\n",
    "- La ley ne doit établir que des peines estrictos y evidentemente necesarios, et nul ne peut être puni qu'en vertu d'une loi établie et promulguée antérieurement au délit, et légalement appliquée.\n",
    "Todo hombre es presumiblemente inocente, ya que es lo que está declarado acoplable, si es jugé indispensable del arresto, todo rigor que no será necesario para asegurar a una persona que debe ser severamente reprimida por la ley.\n",
    "- Nul ne doit être inquiété pour ses opinions, même religieuses, pourvu que su manifestación ne problem pas l'ordre public établi par la loi.\n",
    "\n",
    "RESUMEN:\n",
    "-\n",
    "\"\"\"\n",
    "\n",
    "responses = generate_content(model, contents)\n",
    "print_contents(contents)\n",
    "print_responses(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ri8Yx5vtE22b"
   },
   "source": [
    "… además de pedirle al modelo que ayude con más ideas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 498
    },
    "executionInfo": {
     "elapsed": 4657,
     "status": "ok",
     "timestamp": 1702337413331,
     "user": {
      "displayName": "Laurent Picard",
      "userId": "17424629147771078746"
     },
     "user_tz": -60
    },
    "id": "k5pXqmNJmGN5",
    "outputId": "a8e71ad6-f437-4e18-c557-b62fa38b1c9a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "contents = \"\"\"\n",
    "Proporcione un resumen de 5 puntos clave para una presentación de “chocolate en el mundo”.\n",
    "Una parte debería ser sobre tus orígenes en México (mi maestra tiene familia allí).\n",
    "La última será una degustación con todos los presentes en el aula.\n",
    "\"\"\"\n",
    "\n",
    "# For more creative/diverse answers, let's increase the level of randomness.\n",
    "# Successive requests will likely return different responses.\n",
    "temperature = 0.7\n",
    "top_p = 0.8\n",
    "top_k = 40\n",
    "\n",
    "responses = generate_content(model, contents, temperature, top_p, top_k)\n",
    "\n",
    "print_contents(contents)\n",
    "print_responses(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zf25rsTSkdXu"
   },
   "source": [
    "También puedes solicitar correcciones de texto:\n",
    "\n",
    "A continuación, puede proporcionar algunos ejemplos de respuestas esperadas (es decir, indicaciones breves) para que el modelo pueda comprender qué tipo de respuesta espera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 541
    },
    "executionInfo": {
     "elapsed": 5001,
     "status": "ok",
     "timestamp": 1702337418328,
     "user": {
      "displayName": "Laurent Picard",
      "userId": "17424629147771078746"
     },
     "user_tz": -60
    },
    "id": "jgKEDKVkL8Ub",
    "outputId": "b3c0b14f-e42d-40a1-bf71-bd764acedf4f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "contents = \"\"\"\n",
    "No soy un hablante nativo de inglés.\n",
    "Comprueba que las siguientes frases sean correctas.\n",
    "Cuando sea incorrecto, proporcione una corrección y una explicación.\n",
    "Utilice la misma estructura que los ejemplos dados.\n",
    "EJEMPLOS:\n",
    "- **Hi!**\n",
    "  - Status: ✔️\n",
    "- **Your my best freind!**\n",
    "  - Status: ❌\n",
    "  - Corrección: **You're my best friend!**\n",
    "   - Explicación:\n",
    "     - \"**Your**\" es incorrecto. Parece que intentaste decir \"You're\", que es la forma contraída de \"You are\".\n",
    "     - \"**freind**\" es un error de escritura. La forma correcta es \"**friend**\".\n",
    "\n",
    "ORACIONES:\n",
    "- They're twins, isn't it?\n",
    "- I assisted to the meeting.\n",
    "- You recieved important informations.\n",
    "- I digged a hole in the ice and saw lots of fishes.\n",
    "- That's all folks!\n",
    "\"\"\"\n",
    "responses = generate_content(model, contents)\n",
    "print_contents(contents)\n",
    "print_responses(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ach9JtNikxVz"
   },
   "source": [
    "…además de solicitar contextos más elaborados en varios idiomas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 267
    },
    "executionInfo": {
     "elapsed": 4016,
     "status": "ok",
     "timestamp": 1702337422329,
     "user": {
      "displayName": "Laurent Picard",
      "userId": "17424629147771078746"
     },
     "user_tz": -60
    },
    "id": "P0xdsfQDE22b",
    "outputId": "0b74d75b-fae7-4ac2-dc3c-a03f49802620",
    "tags": []
   },
   "outputs": [],
   "source": [
    "contents = \"\"\"\n",
    "Traduzca el texto a continuación a los idiomas enumerados:\n",
    "\n",
    "TEXTO:\n",
    "Hello folks! I hope you're all doing well. Let's get this workshop started!\n",
    "We'll stick to English because, actually, I can't speak all those languages.\n",
    "\n",
    "IDIOMAS:\n",
    "Español, alemán, griego, búlgaro y japonés.\n",
    "\"\"\"\n",
    "responses = generate_content(model, contents)\n",
    "print_contents(contents)\n",
    "print_responses(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIl7R_jBUsaC"
   },
   "source": [
    "### *Razonamiento* con números"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hm61coMZJX-o"
   },
   "source": [
    "> Nota: Como cualquier LLM, Gemini genera resultados que parecen plausibles, pero aún así pueden resultar alucinantes. Dependiendo de las entradas y parámetros, las salidas pueden ser inexactas, incluidas las operaciones matemáticas. Como práctica recomendada, podría considerar proporcionar al LLM instrucciones paso a paso para reducir las alucinaciones o utilizar una biblioteca de calculadoras en lugar de un LLM.\n",
    "\n",
    "Puedes preguntar sobre problemas de la vida real:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 498
    },
    "executionInfo": {
     "elapsed": 3792,
     "status": "ok",
     "timestamp": 1702337430752,
     "user": {
      "displayName": "Laurent Picard",
      "userId": "17424629147771078746"
     },
     "user_tz": -60
    },
    "id": "nmvIJfDUmGN6",
    "outputId": "44b38862-e1b1-4d5b-a8f1-af360ef66bc9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "contents = \"\"\"\n",
    "Patricia es una excelente corredora y corre una media de 12 km/h.\n",
    "- El lunes corrió durante 1,5 horas. ¿Qué distancia corrió?\n",
    "- El martes corrió 21 km. ¿Cuánto tiempo corrió?\n",
    "- El miércoles corrió 150 minutos. ¿Qué distancia corrió?\n",
    "- A continuación tiene previsto hacer una maratón (42 km). ¿Cuánto tiempo debería tomar?\n",
    "- Para completar un maratón en 3 horas, ¿cuánto más rápido necesita correr?\n",
    "\n",
    "Detalla las respuestas paso a paso.\n",
    "\"\"\"\n",
    "responses = generate_content(model, contents)\n",
    "print_contents(contents)\n",
    "print_responses(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_nZwoSe0mGN6"
   },
   "source": [
    "…y también sobre problemas más clásicos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 160
    },
    "executionInfo": {
     "elapsed": 1519,
     "status": "ok",
     "timestamp": 1702337432266,
     "user": {
      "displayName": "Laurent Picard",
      "userId": "17424629147771078746"
     },
     "user_tz": -60
    },
    "id": "R6dmn9_Q0Be0",
    "outputId": "c574f8a2-6ebd-4e0d-cf5c-af456f93a632",
    "tags": []
   },
   "outputs": [],
   "source": [
    "contents = \"\"\"\n",
    "Acabo de pedirle prestados 1.000 reales a un amigo.\n",
    "Acordamos una tasa de interés simple del 4,5% anual.\n",
    "Quiero saber cuanto tendré que devolver en 1, 2 o 3 años.\n",
    "\"\"\"\n",
    "responses = generate_content(model, contents)\n",
    "print_contents(contents)\n",
    "print_responses(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XBv4oOPpU50E"
   },
   "source": [
    "## Importando el modelo `Gemini 1.0 Pro Vision`\n",
    "\n",
    "El modelo Gemini Pro Vision `gemini-1.0-pro-vision` es un modelo multimodal que admite agregar imágenes y videos al texto o mensajes de chat para una respuesta de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1702337432266,
     "user": {
      "displayName": "Laurent Picard",
      "userId": "17424629147771078746"
     },
     "user_tz": -60
    },
    "id": "ACD_LaIAE22c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "multimodal_model = GenerativeModel(\"gemini-1.0-pro-vision\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvdNYQKJE22c"
   },
   "source": [
    "### *Razonamiento* con una sola imagen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puedes hacer preguntas sencillas sobre la imagen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "executionInfo": {
     "elapsed": 6317,
     "status": "ok",
     "timestamp": 1702337438579,
     "user": {
      "displayName": "Laurent Picard",
      "userId": "17424629147771078746"
     },
     "user_tz": -60
    },
    "id": "aFvAEetIcSA7",
    "outputId": "d4efe8d5-ed41-44e0-eb81-9c26d284738d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"¿Qué aparece en esta imagen?\"\n",
    "# Image by Crissy Jarvis on Unsplash: https://unsplash.com/photos/cHhbULJbPwM\n",
    "image_abacus = load_image_from_url(\n",
    "    \"https://unsplash.com/photos/cHhbULJbPwM/download?w=600\"\n",
    ")\n",
    "\n",
    "contents = [prompt, image_abacus]\n",
    "responses = generate_content(multimodal_model, contents)\n",
    "\n",
    "print_contents(contents)\n",
    "print_responses(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jjqqXuauE22c"
   },
   "source": [
    "…o preguntas específicas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 994
    },
    "executionInfo": {
     "elapsed": 8531,
     "status": "ok",
     "timestamp": 1702337447098,
     "user": {
      "displayName": "Laurent Picard",
      "userId": "17424629147771078746"
     },
     "user_tz": -60
    },
    "id": "H0SOqKjyi1tH",
    "outputId": "6dfb3763-facf-4ffb-c3e0-5e2599efecb6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Responde las siguientes preguntas sobre esta imagen.\n",
    "Devuelve resultados en formato JSON que contienen pares de \"pregunta\" y \"respuesta\".\n",
    "\n",
    "PREGUNTAS:\n",
    "- ¿Qué muestra la imagen?\n",
    "- ¿Como funciona?\n",
    "- ¿Cuándo se invento?\n",
    "- ¿Cómo se llama este objeto en francés, italiano, español, holandés y alemán?\n",
    "- ¿Cuáles son los colores más destacados en la imagen?\n",
    "\"\"\"\n",
    "\n",
    "contents = [prompt, image_abacus]\n",
    "responses = generate_content(multimodal_model, contents)\n",
    "\n",
    "print_contents(contents)\n",
    "print_responses(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I9SIjHnbmGN8"
   },
   "source": [
    "También puede incluir preguntas de *follow-up* en la solicitud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 622
    },
    "executionInfo": {
     "elapsed": 6669,
     "status": "ok",
     "timestamp": 1702337453762,
     "user": {
      "displayName": "Laurent Picard",
      "userId": "17424629147771078746"
     },
     "user_tz": -60
    },
    "id": "bGIsUZFAmGN8",
    "outputId": "69645032-83ea-4748-ff61-10577bc7dc87",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Image by Brett Jordan on Unsplash: https://unsplash.com/photos/E1por_SGvJE\n",
    "image_tiles = load_image_from_url(\n",
    "    \"https://unsplash.com/photos/E1por_SGvJE/download?w=600\"\n",
    ")\n",
    "prompt = \"\"\"\n",
    "- ¿Qué expresión se puede leer en esta imagen? ¿Cómo se presenta?\n",
    "- ¿Cuál es la expresión opuesta?\n",
    "- ¿Cuál es la recomendación, basada en esta expresión, que un profesor podría dar a sus alumnos para un examen?\n",
    "- ¿Con la expresión contraria?\n",
    "\"\"\"\n",
    "\n",
    "contents = [image_tiles, prompt]\n",
    "responses = generate_content(multimodal_model, contents)\n",
    "\n",
    "print_contents(contents)\n",
    "print_responses(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "trT6xm249rqo"
   },
   "source": [
    "La información puede tener diferentes formas. Pueden ser objetos, texto impreso, texto escrito a mano, etc .:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 7922,
     "status": "ok",
     "timestamp": 1702337461673,
     "user": {
      "displayName": "Laurent Picard",
      "userId": "17424629147771078746"
     },
     "user_tz": -60
    },
    "id": "VKsArAoJmGN8",
    "outputId": "2209af8b-8d05-4ba4-d55f-49c16e7d9092",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Sigue las instrucciones.\n",
    "Escribir expresiones matemáticas en LaTex.\n",
    "Utilice una tabla con una fila para cada afirmación y su resultado.\n",
    "\n",
    "INSTRUCCIONES:\n",
    "- Extraer la fórmula.\n",
    "- ¿Cuál es el símbolo justo antes de Pi? ¿Que significa eso?\n",
    "- ¿Es ésta una fórmula famosa? esto tiene nombre?\n",
    "- ¿Porque es especial?\n",
    "- Extraer el subtítulo.\n",
    "- ¿Cuál es el objeto del fondo?\n",
    "- ¿Para qué se usaba?\n",
    "- ¿Cuáles son los colores de la leyenda y la fórmula?\n",
    "\"\"\"\n",
    "\n",
    "image_euler = load_image_from_url(\n",
    "    \"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/math_beauty.jpg\"\n",
    ")\n",
    "\n",
    "contents = [prompt, image_euler]\n",
    "responses = generate_content(multimodal_model, contents)\n",
    "\n",
    "print_contents(contents)\n",
    "print_responses(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHHKFGbMU-Re"
   },
   "source": [
    "También puedes solicitar interpretaciones y sugerencias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 934
    },
    "executionInfo": {
     "elapsed": 9201,
     "status": "ok",
     "timestamp": 1702337470869,
     "user": {
      "displayName": "Laurent Picard",
      "userId": "17424629147771078746"
     },
     "user_tz": -60
    },
    "id": "cJ0TsH-7unIE",
    "outputId": "c7a22ff9-9bd5-4598-a52f-0a526646fdbc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Responde las siguientes preguntas sobre la imagen.\n",
    "Presenta los resultados en una tabla con una línea para cada pregunta y su respuesta.\n",
    "\n",
    "PREGUNTAS:\n",
    "- ¿Qué se ve?\n",
    "- ¿Cuáles son las razones por las que es gracioso?\n",
    "- ¿Cuál podría ser un título divertido?\n",
    "- ¿Qué podría pasar después?\n",
    "- ¿Cómo cambiarías la imagen? ¿Seguiría siendo divertido y por qué?\n",
    "- ¿Cómo harías esto más divertido?\n",
    "\"\"\"\n",
    "\n",
    "image_classroom = load_image_from_url(\n",
    "    \"https://unsplash.com/photos/4ApmfdVo32Q/download?w=600\"\n",
    ")\n",
    "\n",
    "contents = [prompt, image_classroom]\n",
    "responses = generate_content(multimodal_model, contents)\n",
    "\n",
    "print_contents(contents)\n",
    "print_responses(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qU5Tm-FQE22c"
   },
   "source": [
    "### *Razonamiento* con múltiples imágenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vbf1HALvE22d"
   },
   "source": [
    "También puedes utilizar varias imágenes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 7944,
     "status": "ok",
     "timestamp": 1702337478799,
     "user": {
      "displayName": "Laurent Picard",
      "userId": "17424629147771078746"
     },
     "user_tz": -60
    },
    "id": "sLUesezCE22d",
    "outputId": "e85991c8-b5a1-4c0f-f5c3-8b46bd64d765",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Responde las siguientes preguntas para cada imagen.\n",
    "Presenta los resultados en una tabla con una fila para cada imagen y una columna para cada pregunta.\n",
    "\n",
    "PREGUNTAS:\n",
    "- ¿Qué podemos ver en la imagen?\n",
    "- ¿Dónde está?\n",
    "\"\"\"\n",
    "\n",
    "caption_b1 = \"Imagem 1:\"\n",
    "caption_b2 = \"Imagem 2:\"\n",
    "caption_b3 = \"Imagem 3:\"\n",
    "image_b1 = load_image_from_url(\"https://unsplash.com/photos/zzjLGF_6dx4/download?w=600\")\n",
    "image_b2 = load_image_from_url(\"https://unsplash.com/photos/ndAHi2Wxcok/download?w=600\")\n",
    "image_b3 = load_image_from_url(\"https://unsplash.com/photos/5mZ_M06Fc9g/download?w=600\")\n",
    "\n",
    "contents = [prompt, caption_b1, image_b1, caption_b2, image_b2, caption_b3, image_b3]\n",
    "responses = generate_content(multimodal_model, contents)\n",
    "\n",
    "print_contents(contents)\n",
    "print_responses(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CrMHz6A3E22d"
   },
   "source": [
    "…o realizar comparaciones entre imágenes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 8002,
     "status": "ok",
     "timestamp": 1702337486791,
     "user": {
      "displayName": "Laurent Picard",
      "userId": "17424629147771078746"
     },
     "user_tz": -60
    },
    "id": "u7w31hUtE22d",
    "outputId": "f14cd388-e7c3-4d82-d006-b8b94961fe10",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Responde las siguientes preguntas sobre las imágenes, con una respuesta breve y un motivo detallado de tu respuesta.\n",
    "Presenta los resultados en una tabla con una fila para cada pregunta, respuesta y motivo.\n",
    "\n",
    "PREGUNTAS:\n",
    "- ¿Qué tienen en común las imágenes?\n",
    "- ¿Cuál le interesaría a un matemático?\n",
    "- ¿Cuál indica que es el fin de las vacaciones?\n",
    "- ¿Cuál sugiere que podamos tomar un café allí?\n",
    "\"\"\"\n",
    "\n",
    "contents = [prompt, caption_b1, image_b1, caption_b2, image_b2, caption_b3, image_b3]\n",
    "responses = generate_content(multimodal_model, contents)\n",
    "\n",
    "print_contents(contents)\n",
    "print_responses(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uqFSjI4_mGN9"
   },
   "source": [
    "Puedes utilizar el nivel de lenguaje y comprensión visual de Gemini para trabajar en conceptos o incluso obtener sugerencias para nuevas imágenes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 6093,
     "status": "ok",
     "timestamp": 1702337492875,
     "user": {
      "displayName": "Laurent Picard",
      "userId": "17424629147771078746"
     },
     "user_tz": -60
    },
    "id": "QjbP2y26mGN9",
    "outputId": "7a072645-e7ff-4049-93a7-f27bb6e27c7c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Responde las siguientes preguntas sobre las imágenes, con una respuesta breve y un motivo detallado de tu respuesta.\n",
    "Presenta los resultados en una tabla con una fila para cada pregunta, respuesta y motivo.\n",
    "\n",
    "PREGUNTAS:\n",
    "- ¿Qué representa la primera imagen?\n",
    "- ¿Qué representa la segunda imagen?\n",
    "- ¿Cuál podría ser la siguiente imagen lógica?\n",
    "\"\"\"\n",
    "\n",
    "caption_w1 = \"Imagen 1:\"\n",
    "caption_w2 = \"Imagen 2:\"\n",
    "image_w1 = load_image_from_url(\"https://unsplash.com/photos/TA5bUTySOrg/download?w=600\")\n",
    "image_w2 = load_image_from_url(\"https://unsplash.com/photos/Nw_D8v79PM4/download?w=600\")\n",
    "\n",
    "contents = [prompt, caption_w1, image_w1, caption_w2, image_w2]\n",
    "responses = generate_content(multimodal_model, contents)\n",
    "\n",
    "print_contents(contents)\n",
    "print_responses(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dJdBiddamGN9"
   },
   "source": [
    "Y el límite es tu imaginación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 9644,
     "status": "ok",
     "timestamp": 1702337502502,
     "user": {
      "displayName": "Laurent Picard",
      "userId": "17424629147771078746"
     },
     "user_tz": -60
    },
    "id": "iFhubVwFmGN9",
    "outputId": "4f2f0ada-b96b-41c2-d2ba-9f0d1e3042c9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Responda las siguientes preguntas, con una respuesta breve y un motivo detallado de su respuesta.\n",
    "Presenta los resultados en una tabla con una fila para cada pregunta, respuesta y motivo.\n",
    "\n",
    "PREGUNTAS:\n",
    "- ¿Qué tema ilustran estas imágenes?\n",
    "- ¿Cuál podría ser otra imagen que sustituya a la primera?\n",
    "- ¿Qué otra imagen podría sustituir a la segunda?\n",
    "- ¿Cuál sería una alternativa a la tercera imagen?\n",
    "- ¿Y para el último?\n",
    "\"\"\"\n",
    "\n",
    "caption_s1 = \"Imagem 1:\"\n",
    "caption_s2 = \"Imagem 2:\"\n",
    "caption_s3 = \"Imagem 3:\"\n",
    "caption_s4 = \"Imagem 4:\"\n",
    "image_s1 = load_image_from_url(\"https://unsplash.com/photos/eriuKJwcdjI/download?w=600\")\n",
    "image_s2 = load_image_from_url(\"https://unsplash.com/photos/QldMpmrmWuc/download?w=600\")\n",
    "image_s3 = load_image_from_url(\"https://unsplash.com/photos/rN3m7aTH3io/download?w=600\")\n",
    "image_s4 = load_image_from_url(\"https://unsplash.com/photos/FhdN5QVrBfY/download?w=600\")\n",
    "\n",
    "contents = [\n",
    "    prompt,\n",
    "    caption_s1,\n",
    "    image_s1,\n",
    "    caption_s2,\n",
    "    image_s2,\n",
    "    caption_s3,\n",
    "    image_s3,\n",
    "    caption_s4,\n",
    "    image_s4,\n",
    "]\n",
    "responses = generate_content(multimodal_model, contents)\n",
    "\n",
    "print_contents(contents)\n",
    "print_responses(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JkjzQsgKGS7o"
   },
   "source": [
    "### *Razonamiento* con vídeos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 790
    },
    "executionInfo": {
     "elapsed": 24732,
     "status": "ok",
     "timestamp": 1702337527230,
     "user": {
      "displayName": "Laurent Picard",
      "userId": "17424629147771078746"
     },
     "user_tz": -60
    },
    "id": "o9dOaKJoUknq",
    "outputId": "a870895c-ca0a-4003-e641-2f3485678e30",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Responde las siguientes preguntas utilizando únicamente el vídeo.\n",
    "Presenta los resultados en una tabla con una línea para cada pregunta y su respuesta.\n",
    "\n",
    "PREGUNTAS:\n",
    "- ¿Cuál es el animal principal visible a lo largo del video?\n",
    "- ¿Qué dispositivos electrónicos son visibles?\n",
    "- ¿A qué animales se toman selfies en primer plano los personajes de dibujos animados?\n",
    "- ¿Qué marcas famosas son visibles?\n",
    "- ¿Cuál es el texto visible al final?\n",
    "\"\"\"\n",
    "\n",
    "video = Part.from_uri(\n",
    "    uri=\"gs://cloud-samples-data/video/animals.mp4\",\n",
    "    mime_type=\"video/mp4\",\n",
    ")\n",
    "\n",
    "contents = [prompt, video]\n",
    "responses = generate_content(multimodal_model, contents)\n",
    "\n",
    "print_contents(contents)\n",
    "print_responses(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jWT14Lw6Uknq"
   },
   "source": [
    "## Conclusión\n",
    "\n",
    "En este tutorial, viste ejemplos de cómo usar Gemini para la educación y beneficiarte de modelos textuales y multimodales para generar contenido a partir de textos, imágenes y videos.\n",
    "\n",
    "También puede explorar otros tutoriales que se centran en diferentes dominios o detalles específicos de la API Vertex AI Gemini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m117",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m117"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
