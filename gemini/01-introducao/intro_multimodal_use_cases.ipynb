{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijGzTHJJUCPY",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Copyright 2023 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEqbX8OhE8y9",
        "tags": []
      },
      "source": [
        "# Gemini: Una descripción general de los escenarios multimodales\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/intro_multimodal_use_cases.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Ejecutar en Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/intro_multimodal_use_cases.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> Ver en GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/intro_multimodal_use_cases.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Abrir en Workbench de Vertex AI\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VK1Q5ZYdVL4Y"
      },
      "source": [
        "## Visión General\n",
        "\n",
        "### Gemini\n",
        "\n",
        "Gemini es una familia de modelos generativos de IA desarrollados por Google DeepMind y proyectados para casos de uso multimodales. La API Gemini da acceso a los modelos Gemini Pro Vision y Gemini Pro\n",
        "\n",
        "\n",
        "### API Vertex AI Gemini\n",
        "\n",
        "La API Vertex AI Gemini nos da una interface unificada para interactuar com modelos Gemini. Actualmente existen dos modelos disponíbles en la API Gemini:\n",
        "\n",
        "\n",
        "- **Modelo Gemini Pro** (`gemini-pro`): Proyectado para manejar tareas de lenguage natural, chat multiturno de texto y código y generación de código.\n",
        "- **Modelo Gemini Pro Vision** (`gemini-pro-vision`): Soporta prompts multimodales. Puedes incluir texto, imágenes y video en los prompt y obtener respuestas en texto o código.\n",
        "\n",
        "Puedes interactuar con la API Gemini usando los siguiente métodos:\n",
        "\n",
        "- Usando [Vertex AI Studio](https://cloud.google.com/generative-ai-studio) para pruebas rápidos y generación de comandos.\n",
        "- Usando el SDK de Vertex AI\n",
        "\n",
        "Este notebook se concntra en el uso de **Vertex AI SDK para Python** para utilizar la API Vertex AI Gemini.\n",
        "\n",
        "Para obtener más inforción, consulte la documentación [IA Generativa en Vertex AI](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/overview)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQT500QqVPIb"
      },
      "source": [
        "### Objetivos\n",
        "\n",
        "Este notebook demuestra una variedad de casos de uso multimodal para los que se puede utilizar Gemini.\n",
        "\n",
        "#### Casos de uso multimodales\n",
        "\n",
        "En comparación con los LLM de solo texto, la multimodalidad de Gemini Pro Vision se puede utilizar para muchos casos de uso nuevos:\n",
        "\n",
        "Ejemplos de casos de uso con **texto e imágenes** como entrada:\n",
        "\n",
        "- Detectar objetos en fotografías\n",
        "- Comprender las pantallas y las interfaces\n",
        "- Comprensión del dibujo y la abstracción.\n",
        "- Comprender gráficos y diagramas.\n",
        "- Recomendación de imagen basada en las preferencias del usuario.\n",
        "- Comparar imágenes en busca de similitudes, anomalías o diferencias.\n",
        "\n",
        "Ejemplos de casos de uso con **texto y video** como entrada:\n",
        "\n",
        "- Generando una descripción de video\n",
        "- Extraer etiquetas de objetos a lo largo de un vídeo\n",
        "- Extraer momentos destacados/mensajes de un vídeo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhsUe0fyc-ER"
      },
      "source": [
        "### Costos\n",
        "\n",
        "Este tutorial usa los siguientes componentes de Google Cloud que pueden generar costos en su factura:\n",
        "\n",
        "- Vertex AI\n",
        "\n",
        "Para mayores detalles puedes revisar los [precios de Vertex AI](https://cloud.google.com/vertex-ai/pricing) y usar la [calculadora de precios](https://cloud.google.com/products/calculator/) para generar una estimativa de costos con base en el uso del proyecto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDU0XJ1xRDlL"
      },
      "source": [
        "## Primeros pasos\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5afkyDMSBW5"
      },
      "source": [
        "### Instala el SDK de Vertex AI\n",
        "**Importante:** solo descomente la siguiente línea si **no estas** ejecutando este laboratório en Qwiklabs (Skills Boost)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kc4WxYmLSBW5",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# ! pip3 install --upgrade --user google-cloud-aiplatform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7UyNVSiyQ96"
      },
      "source": [
        "### **Reinicie el kernel de su jupyter notebook**\n",
        "\n",
        "Como la instalción está siendo realizada con la opción `--user` es necesario reiniciar el kernel para que los nuevos módulos se tornen accesibles.\n",
        "\n",
        "**Importante:** solo descomente las siguientes lineas si **não estiver** ejecutando este laboratório en Qwiklabs (Skills Boost)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmY9HVVGSBW5",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# import IPython\n",
        "\n",
        "# app = IPython.Application.instance()\n",
        "# app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "8lxKg8Wni9WF"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ El kernel del notebook está siendo reiniciado. Por favor espere que este proceso sea finalizado para poder continuar con los próximos pasos. ⚠️</b>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Fom0ZkMSBW6"
      },
      "source": [
        "### **Solamente para uso en Colab - Autentique su ambiente de notebook**\n",
        "\n",
        "En el caso que estes ejecutando este notebook en un Google Colab, descomenta la siguiente célula para realizar la autenticación de su sesión de notebook con Google Cloud. Este paso es importante **para la utilización del Colab**, asi podemos garantizar que las llamdas a las APIs de Google Cloud funcionen sin problemas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCaCx6PLSBW6"
      },
      "outputs": [],
      "source": [
        "# import sys\n",
        "\n",
        "# # Additional authentication is required for Google Colab\n",
        "# if \"google.colab\" in sys.modules:\n",
        "#     # Authenticate user to Google Cloud\n",
        "#     from google.colab import auth\n",
        "\n",
        "#     auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGB8Txa_e4V0"
      },
      "source": [
        "### **Solamente para usos en Colab - define el proyecto en Google Cloud que va a ser utilizado**\n",
        "\n",
        "En el caso que estes ejeccutando este notebook en un Google Colab, descomente la siguiente célula para definir que proyecto en Google Cloud sera utilizado por el Colab en la ejecución de este notebook. De lo contrario, continue con los próximos pasos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGOJHtgDe5-r"
      },
      "outputs": [],
      "source": [
        "# if \"google.colab\" in sys.modules:\n",
        "#     # Define project information\n",
        "#     PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
        "#     LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "\n",
        "#     # Initialize Vertex AI\n",
        "#     import vertexai\n",
        "\n",
        "#     vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuQwwRiniVFG"
      },
      "source": [
        "### Importe las bibliotecas necesarias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTk488WDPBtQ",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\", UserWarning)\n",
        "\n",
        "from vertexai.generative_models import (\n",
        "    GenerationConfig,\n",
        "    GenerativeModel,\n",
        "    Image,\n",
        "    Part,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7rZuTClfNs0"
      },
      "source": [
        "## Importe el modelo `Gemini 1.0 Pro Vision`\n",
        "\n",
        "Gemini Pro Vision (`gemini-1.0-pro-vision`) es un modelo multimodal que admite indicaciones multimodales. Puede incluir texto, imágenes y videos en sus solicitudes de avisos y obtener respuestas en texto o código."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2998506fe6d1",
        "tags": []
      },
      "outputs": [],
      "source": [
        "multimodal_model = GenerativeModel(\"gemini-1.0-pro-vision\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpL3OkSCfIAR"
      },
      "source": [
        "### Define algunas funciones auxiliares\n",
        "\n",
        "Define funciones auxiliares para cargar y mostrar imágenes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7QMAHXse339",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import http.client\n",
        "import typing\n",
        "import urllib.request\n",
        "\n",
        "import IPython.display\n",
        "from PIL import Image as PIL_Image\n",
        "from PIL import ImageOps as PIL_ImageOps\n",
        "\n",
        "\n",
        "def display_images(\n",
        "    images: typing.Iterable[Image],\n",
        "    max_width: int = 600,\n",
        "    max_height: int = 350,\n",
        ") -> None:\n",
        "    for image in images:\n",
        "        pil_image = typing.cast(PIL_Image.Image, image._pil_image)\n",
        "        if pil_image.mode != \"RGB\":\n",
        "            # RGB is supported by all Jupyter environments (e.g. RGBA is not yet)\n",
        "            pil_image = pil_image.convert(\"RGB\")\n",
        "        image_width, image_height = pil_image.size\n",
        "        if max_width < image_width or max_height < image_height:\n",
        "            # Resize to display a smaller notebook image\n",
        "            pil_image = PIL_ImageOps.contain(pil_image, (max_width, max_height))\n",
        "        IPython.display.display(pil_image)\n",
        "\n",
        "\n",
        "def get_image_bytes_from_url(image_url: str) -> bytes:\n",
        "    with urllib.request.urlopen(image_url) as response:\n",
        "        response = typing.cast(http.client.HTTPResponse, response)\n",
        "        image_bytes = response.read()\n",
        "    return image_bytes\n",
        "\n",
        "\n",
        "def load_image_from_url(image_url: str) -> Image:\n",
        "    image_bytes = get_image_bytes_from_url(image_url)\n",
        "    return Image.from_bytes(image_bytes)\n",
        "\n",
        "\n",
        "def display_content_as_image(content: str | Image | Part) -> bool:\n",
        "    if not isinstance(content, Image):\n",
        "        return False\n",
        "    display_images([content])\n",
        "    return True\n",
        "\n",
        "\n",
        "def display_content_as_video(content: str | Image | Part) -> bool:\n",
        "    if not isinstance(content, Part):\n",
        "        return False\n",
        "    part = typing.cast(Part, content)\n",
        "    file_path = part.file_data.file_uri.removeprefix(\"gs://\")\n",
        "    video_url = f\"https://storage.googleapis.com/{file_path}\"\n",
        "    IPython.display.display(IPython.display.Video(video_url, width=600))\n",
        "    return True\n",
        "\n",
        "\n",
        "def print_multimodal_prompt(contents: list[str | Image | Part]):\n",
        "    \"\"\"\n",
        "    Given contents that would be sent to Gemini,\n",
        "    output the full multimodal prompt for ease of readability.\n",
        "    \"\"\"\n",
        "    for content in contents:\n",
        "        if display_content_as_image(content):\n",
        "            continue\n",
        "        if display_content_as_video(content):\n",
        "            continue\n",
        "        print(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OWurhO4mu4J"
      },
      "source": [
        "## Comprender el contexto en varias imágenes\n",
        "\n",
        "Una de las capacidades de Gemini es poder razonar a través de múltiples imágenes.\n",
        "\n",
        "Aquí hay un ejemplo del uso de Gemini para calcular el costo total de los alimentos usando una imagen de frutas y una lista de precios:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UyRoVquPmy9H",
        "tags": []
      },
      "outputs": [],
      "source": [
        "image_grocery_url = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/banana-apple.jpg\"\n",
        "image_prices_url = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/pricelist.jpg\"\n",
        "image_grocery = load_image_from_url(image_grocery_url)\n",
        "image_prices = load_image_from_url(image_prices_url)\n",
        "\n",
        "instructions = \"Instrucciones: Considere la siguiente imagen que contiene frutas.:\"\n",
        "prompt1 = \"¿Cuánto pagaré por las frutas considerando esta lista de precios?\"\n",
        "prompt2 = \"\"\"\n",
        "Responda la pregunta a través de estos pasos:\n",
        "Paso 1: Identifica qué tipo de fruta hay en la primera imagen.\n",
        "Paso 2: Cuenta la cantidad de cada fruta.\n",
        "Paso 3: Para cada artículo en la primera imagen, verifique el precio del artículo en la tabla de precios.\n",
        "Paso 4: Calcula el precio subtotal de cada tipo de fruta.\n",
        "Paso 5: Calcula el precio total de las frutas usando los subtotales.\n",
        "Paso 6: si pago con un billete de 20 reales, ¿cuánto cambio recibiré?\n",
        "\n",
        "Responda y describa los pasos seguidos:\"\"\"\n",
        "\n",
        "contents = [\n",
        "    instructions,\n",
        "    image_grocery,\n",
        "    prompt1,\n",
        "    image_prices,\n",
        "    prompt2,\n",
        "]\n",
        "\n",
        "responses = multimodal_model.generate_content(contents, stream=True)\n",
        "\n",
        "print(\"-------Prompt--------\")\n",
        "print_multimodal_prompt(contents)\n",
        "\n",
        "print(\"\\n-------Respuesta--------\")\n",
        "for response in responses:\n",
        "    print(response.text, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zy-me3PdgMUH",
        "tags": []
      },
      "source": [
        "## Comprender las pantallas y las interfaces\n",
        "\n",
        "Gemini también puede extraer información de las pantallas de los dispositivos, interfaces de usuario, capturas de pantalla, íconos y diseños.\n",
        "\n",
        "Por ejemplo, si ingresa una imagen de una estufa, puede pedirle a Gemini que proporcione instrucciones para ayudar al usuario a navegar por la interfaz y responder en diferentes idiomas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDjN4thV8orx",
        "tags": []
      },
      "outputs": [],
      "source": [
        "image_stove_url = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/stove.jpg\"\n",
        "image_stove = load_image_from_url(image_stove_url)\n",
        "\n",
        "prompt = \"\"\"¿Cómo puedo configurar el reloj en este dispositivo?\n",
        "Si las instrucciones incluyen botones, explique también dónde están ubicados físicamente esos botones.\n",
        "Proporcionar tres versiones de instrucciones: en portugués, inglés y francés.\n",
        "\"\"\"\n",
        "\n",
        "contents = [image_stove, prompt]\n",
        "\n",
        "responses = multimodal_model.generate_content(contents, stream=True)\n",
        "\n",
        "print(\"-------Prompt--------\")\n",
        "print_multimodal_prompt(contents)\n",
        "\n",
        "print(\"\\n-------Respuesta--------\")\n",
        "for response in responses:\n",
        "    print(response.text, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbhfMO478orx"
      },
      "source": [
        "**Nota:** Es posible que la respuesta no sea del todo precisa, ya que el modelo puede estar alucinando; sin embargo, el modelo es capaz de identificar la ubicación de los botones y traducirla en una única consulta. Para mitigar las alucinaciones, un enfoque es fundamentar el LLM con técnicas como RAG, que está fuera del alcance de este notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4437b7608c8e"
      },
      "source": [
        "## Comprensión de las relaciones entre entidades en diagramas técnicos\n",
        "\n",
        "Gemini tiene capacidades multimodales que le permiten comprender diagramas y realizar pasos prácticos como optimización o generación de código. Este ejemplo demuestra cómo Gemini puede descifrar un diagrama de entidad-relación (ER), comprender las relaciones entre tablas, identificar requisitos de optimización en un entorno específico como BigQuery e incluso generar el código correspondiente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klY4yBEiKmET"
      },
      "outputs": [],
      "source": [
        "image_er_url = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/er.png\"\n",
        "image_er = load_image_from_url(image_er_url)\n",
        "\n",
        "prompt = \"Documente las entidades y relaciones en este diagrama de entidad-relación (DER).\"\n",
        "\n",
        "contents = [prompt, image_er]\n",
        "\n",
        "# Use a more deterministic configuration with a low temperature\n",
        "generation_config = GenerationConfig(\n",
        "    temperature=0.1,\n",
        "    top_p=0.8,\n",
        "    top_k=40,\n",
        "    candidate_count=1,\n",
        "    max_output_tokens=2048,\n",
        ")\n",
        "\n",
        "responses = multimodal_model.generate_content(\n",
        "    contents,\n",
        "    generation_config=generation_config,\n",
        "    stream=True,\n",
        ")\n",
        "\n",
        "print(\"-------Prompt--------\")\n",
        "print_multimodal_prompt(contents)\n",
        "\n",
        "print(\"\\n-------Respuesta--------\")\n",
        "for response in responses:\n",
        "    print(response.text, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXrvQxpiQKp6"
      },
      "source": [
        "## Recomendaciones basadas en múltiples imágenes.\n",
        "\n",
        "Gemini puede comparar imágenes y proporcionar recomendaciones. Esto puede resultar útil en sectores como el comercio electrónico y el comercio minorista.\n",
        "\n",
        "A continuación se muestra un ejemplo de cómo elegir qué par de gafas serían las más adecuadas para una cara ovalada:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7M7B9q7L1X7"
      },
      "outputs": [],
      "source": [
        "image_glasses1_url = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/glasses1.jpg\"\n",
        "image_glasses2_url = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/glasses2.jpg\"\n",
        "image_glasses1 = load_image_from_url(image_glasses1_url)\n",
        "image_glasses2 = load_image_from_url(image_glasses2_url)\n",
        "\n",
        "prompt1 = \"\"\"\n",
        "¿Cuál de estas gafas me recomiendas según la forma de mi cara?\n",
        "Tengo una cara ovalada.\n",
        "----\n",
        "Gafas 1:\n",
        "\"\"\"\n",
        "prompt2 = \"\"\"\n",
        "----\n",
        "Gafas 2:\n",
        "\"\"\"\n",
        "prompt3 = \"\"\"\n",
        "----\n",
        "Explica cómo tomas esta decisión.\n",
        "Proporcione su recomendación según la forma de mi cara y el razonamiento para cada una.\n",
        "Proporcione la respuesta en formato JSON.\n",
        "\"\"\"\n",
        "\n",
        "contents = [prompt1, image_glasses1, prompt2, image_glasses2, prompt3]\n",
        "\n",
        "responses = multimodal_model.generate_content(contents, stream=True)\n",
        "\n",
        "print(\"-------Prompt--------\")\n",
        "print_multimodal_prompt(contents)\n",
        "\n",
        "print(\"\\n-------Respuesta--------\")\n",
        "for response in responses:\n",
        "    print(response.text, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBrdsvIU7Zkf"
      },
      "source": [
        "## Similitudes y diferencias\n",
        "\n",
        "Los modelos Géminis pueden comparar imágenes e identificar similitudes o diferencias entre objetos.\n",
        "\n",
        "El siguiente ejemplo muestra dos escenas de Marienplatz en Munich, Alemania, que son ligeramente diferentes. Géminis puede comparar imágenes y encontrar similitudes/diferencias:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUSJduLh8457"
      },
      "outputs": [],
      "source": [
        "image_landmark1_url = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/landmark1.jpg\"\n",
        "image_landmark2_url = \"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/landmark2.jpg\"\n",
        "image_landmark1 = load_image_from_url(image_landmark1_url)\n",
        "image_landmark2 = load_image_from_url(image_landmark2_url)\n",
        "\n",
        "prompt1 = \"\"\"\n",
        "Considere las siguientes imágenes:\n",
        "Imagen 1:\n",
        "\"\"\"\n",
        "prompt2 = \"\"\"\n",
        "Imagen 2:\n",
        "\"\"\"\n",
        "prompt3 = \"\"\"\n",
        "Responde las siguientes preguntas en una frase corta.\n",
        "\n",
        "1. ¿Qué se muestra en la Imagen 1?\n",
        "2. ¿En qué se parecen las dos imágenes?\n",
        "3. ¿Cuál es la diferencia entre la Imagen 1 y la Imagen 2 en términos de contenido o personas mostradas?\"\"\"\n",
        "\n",
        "contents = [prompt1, image_landmark1, prompt2, image_landmark2, prompt3]\n",
        "\n",
        "generation_config = GenerationConfig(\n",
        "    temperature=0.0,\n",
        "    top_p=0.2,\n",
        "    top_k=40,\n",
        "    candidate_count=1,\n",
        "    max_output_tokens=2048,\n",
        ")\n",
        "\n",
        "responses = multimodal_model.generate_content(\n",
        "    contents,\n",
        "    generation_config=generation_config,\n",
        "    stream=True,\n",
        ")\n",
        "\n",
        "print(\"-------Prompt--------\")\n",
        "print_multimodal_prompt(contents)\n",
        "\n",
        "print(\"\\n-------Respuesta--------\")\n",
        "for response in responses:\n",
        "    print(response.text, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dN8nVlITK5kz"
      },
      "source": [
        "## Generando una descripción de video\n",
        "\n",
        "Gemini también puede extraer etiquetas de un vídeo:\n",
        "\n",
        "> Video: https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/mediterraneansea.mp4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tT2nArvxZv-P"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "¿Qué se muestra en este vídeo?\n",
        "¿Dónde debería ir para ver esto?\n",
        "¿Cuáles son otros 5 lugares en el mundo que se parecen a este?\n",
        "\"\"\"\n",
        "\n",
        "video = Part.from_uri(\n",
        "    uri=\"gs://github-repo/img/gemini/multimodality_usecases_overview/mediterraneansea.mp4\",\n",
        "    mime_type=\"video/mp4\",\n",
        ")\n",
        "contents = [prompt, video]\n",
        "\n",
        "responses = multimodal_model.generate_content(contents, stream=True)\n",
        "\n",
        "print(\"-------Prompt--------\")\n",
        "print_multimodal_prompt(contents)\n",
        "\n",
        "print(\"\\n-------Respuesta--------\")\n",
        "for response in responses:\n",
        "    print(response.text, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9RdLpH128Ao"
      },
      "source": [
        "> Puede confirmar que la ubicación es Antalya, Turquía, visitando la página de Wikipedia: https://en.wikipedia.org/wiki/Antalya"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksjZiIfnO0zQ"
      },
      "source": [
        "## Extraer etiquetas de objetos a lo largo del vídeo\n",
        "\n",
        "Gemini también puede extraer etiquetas de un vídeo.\n",
        "\n",
        "> Video: https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/photography.mp4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9qE2GGIA975"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "Responde las siguientes preguntas usando solo el video:\n",
        "- ¿Qué hay en el vídeo?\n",
        "- ¿Cuál es la acción en el video?\n",
        "- ¿Proporcionas las 10 etiquetas principales para este vídeo?\n",
        "\"\"\"\n",
        "\n",
        "video = Part.from_uri(\n",
        "    uri=\"gs://github-repo/img/gemini/multimodality_usecases_overview/photography.mp4\",\n",
        "    mime_type=\"video/mp4\",\n",
        ")\n",
        "contents = [prompt, video]\n",
        "\n",
        "responses = multimodal_model.generate_content(contents, stream=True)\n",
        "\n",
        "print(\"-------Prompt--------\")\n",
        "print_multimodal_prompt(contents)\n",
        "\n",
        "print(\"\\n-------Respuesta--------\")\n",
        "for response in responses:\n",
        "    print(response.text, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiQzvIJfQxbQ"
      },
      "source": [
        "## Hacer más preguntas sobre un vídeo\n",
        "\n",
        "A continuación se muestra otro ejemplo para usar con Gemini en este escenario de preguntas en video.\n",
        "\n",
        "> Video: https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/pixel8.mp4\n",
        "> _Nota: Aunque este video contiene audio, Gemini actualmente no admite entrada de audio y solo responderá según el video._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQIV9SwCz5WM"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "Responde las siguientes preguntas usando solo el video:\n",
        "¿Cuál es la profesión de la persona principal?\n",
        "¿Cuáles son las principales características del teléfono destacadas?\n",
        "¿En qué ciudad se registró esto?\n",
        "Proporcione la respuesta JSON.\n",
        "\"\"\"\n",
        "\n",
        "video = Part.from_uri(\n",
        "    uri=\"gs://github-repo/img/gemini/multimodality_usecases_overview/pixel8.mp4\",\n",
        "    mime_type=\"video/mp4\",\n",
        ")\n",
        "contents = [prompt, video]\n",
        "\n",
        "responses = multimodal_model.generate_content(contents, stream=True)\n",
        "\n",
        "print(\"-------Prompt--------\")\n",
        "print_multimodal_prompt(contents)\n",
        "\n",
        "print(\"\\n-------Respuesta--------\")\n",
        "for response in responses:\n",
        "    print(response.text, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LurOmNuRpDr"
      },
      "source": [
        "## Recuperar información adicional más allá del vídeo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xTrnKtIi9WU"
      },
      "source": [
        "> Video: https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/ottawatrain3.mp4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CY-zlixU87O"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "¿Qué línea es esta?\n",
        "¿a dónde va esto?\n",
        "¿Cuáles son las estaciones/paradas?\n",
        "\"\"\"\n",
        "video = Part.from_uri(\n",
        "    uri=\"gs://github-repo/img/gemini/multimodality_usecases_overview/ottawatrain3.mp4\",\n",
        "    mime_type=\"video/mp4\",\n",
        ")\n",
        "contents = [prompt, video]\n",
        "\n",
        "responses = multimodal_model.generate_content(contents, stream=True)\n",
        "\n",
        "print(\"-------Prompt--------\")\n",
        "print_multimodal_prompt(contents)\n",
        "\n",
        "print(\"\\n-------Respuesta--------\")\n",
        "for response in responses:\n",
        "    print(response.text, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZrxMm_83Vps"
      },
      "source": [
        "> Puedes confirmar que esta es efectivamente la Línea Confederada en Wikipedia aquí: https://en.wikipedia.org/wiki/Confederation_Line"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-pL6oDZi9WU"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "environment": {
      "kernel": "python3",
      "name": "tf2-cpu.2-11.m117",
      "type": "gcloud",
      "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m117"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}